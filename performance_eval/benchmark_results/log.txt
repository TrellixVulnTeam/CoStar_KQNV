01_15_20_benchmark_results.json / 01_15_20_benchmark_results.eps
================================================================
- results of performance evaluation on Nobel data set
- each bar is average of 10 parser runs on a single file

01_15_20_perf_report.png
========================
- result of profiling code with perf tool
- shows that mapping over the entire parser stack to extract the location
  from each frame takes up most of the computation time



01_22_20_benchmark_results_after_stack_sep.eps
==============================================
- two stacks: grammar locations and semantic values
- average of 3 parser runs on each Nobel file
- runtime on largest file dropped from 20s to 4s

01_22_20_perf_report_after_stack_sep.png
========================================
- most expensive operations are spClosureStep, add (maybe from adding to av
set), and app



01_27_20_benchmark_results_prefix_and_suffix_stacks.eps
=======================================================
- two stacks: gamma prefixes / semantic values, unprocessed suffixes
- time drops to 2.5 seconds on largest file



01_28_20_remove_parser_appends.eps
==================================
- consing instead of semantic values maybe gives a (very slight) improvement



02_12_20_user_defined_symbol_types.eps
======================================
- switching from nat nonterminals and string terminals to user-defined grammar
  symbol types yields about a 10% improvement



02_14_20_no_avail_set_in_subparser.eps
======================================
- remove available set from subparser record type, make it a separate parameter
  to prediction-related functions; very little performance difference



03_02_20_initial_cache_attempt.eps
==================================
- early version of DFA cache optimization is unsuccessful; failing over whenever
  a subparser dips into the context results in a slow-growing cache and too many
  failovers


03_09_20_compute_closure_statically.eps
=======================================
- an optimized version of prediction that involves computing the closure sites
of all grammar positions statically results in a big performance improvement


03_14_20_dynamic_cache.eps
==========================
- building a cache dynamically also results in a big performance improvement --
  slightly smaller than the static optimization mentioned above, but the dynamic
  optimization is a better approximation of LL prediction, meaning that it will
  fail over to LL prediction less often
